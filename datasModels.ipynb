{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# coding=utf-8\n",
    "# get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers.core import Flatten\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dropout,LSTM,BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./Pistachio_Dataset/Pistachio_28_Features_Dataset/Pistachio_28_Features_Dataset.csv') \n",
    "cols = ['Area', 'Perimeter', 'Major_Axis', 'Minor_Axis', 'Eccentricity',\n",
    "        'Eqdiasq', 'Solidity', 'Convex_Area', 'Extent', 'Aspect_Ratio',\n",
    "        'Roundness', 'Compactness', 'Shapefactor_1', 'Shapefactor_2',\n",
    "        'Shapefactor_3', 'Shapefactor_4', 'Mean_RR', 'Mean_RG', 'Mean_RB',\n",
    "        'StdDev_RR', 'StdDev_RG', 'StdDev_RB', 'Skew_RR', 'Skew_RG', 'Skew_RB',\n",
    "        'Kurtosis_RR', 'Kurtosis_RG', 'Kurtosis_RB','Class']\n",
    "# dataset = dataset.loc[:,cols]\n",
    "# d = {'Class': dataset['Class'].value_counts().index, 'count': dataset['Class'].value_counts()}\n",
    "# df_Class = pd.DataFrame(data=d).reset_index(drop=True)\n",
    "# df_Class\n",
    "dataset.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FReIndex(colIndex,reIndex):\n",
    "    dirindex = {}\n",
    "    lisindex = []\n",
    "    for i in range(len(colIndex)):\n",
    "        lisindex.append(i)\n",
    "    dirindex = dict(zip(lisindex,colIndex))\n",
    "    colIndex = []\n",
    "    for i in range(len(reIndex)):\n",
    "        for key in dirindex:\n",
    "            if int(key) == int(reIndex[i]):\n",
    "                colIndex.append(dirindex[key])\n",
    "                break\n",
    "    print(colIndex)\n",
    "    return colIndex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lisclass = dataset['Class'].values\n",
    "LisClassNp = []\n",
    "for i in range(len(Lisclass)):\n",
    "    if (Lisclass[i] == 'Kirmizi_Pistachio'):\n",
    "        LisClassNp.append([0,1])\n",
    "    else:\n",
    "        LisClassNp.append([1,0])\n",
    "Y = np.array(LisClassNp)\n",
    "dataset = dataset.drop(columns='Class')\n",
    "# lisindex = []\n",
    "# for i in range(len(dataset.columns)):\n",
    "#     lisindex.append(i)\n",
    "# dataset = dataset.reindex(lisindex)\n",
    "colIndex = ['Area','Minor_Axis', 'Eccentricity',\n",
    "            'Eqdiasq','Convex_Area', 'Aspect_Ratio',\n",
    "            'Compactness', 'Shapefactor_1', \n",
    "            'Shapefactor_3']\n",
    "\n",
    "lis = [] #优化算法参数入口（编码）\n",
    "for i in range(len(colIndex)):\n",
    "    lis.append(len(colIndex)-i)\n",
    "    \n",
    "#colIndex = FReIndex(colIndex,lis)\n",
    "dataset = pd.DataFrame(dataset,columns=colIndex)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "#拆分训练集和测试集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=40)\n",
    "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os,sys\n",
    "import shutil\n",
    "\n",
    "shutil.rmtree('models')  \n",
    "os.mkdir('models')\n",
    "\n",
    "Note = open('restult.txt',mode='w')\n",
    "Note.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "\n",
    "sgd = SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "lstm_layers = [1,2,3,4,5]\n",
    "dense_layers = [1,2,3,4,5,6]\n",
    "units = [16,32,64,128]\n",
    "dropout = [0.05,0.1,0.15,0.25]\n",
    "Batch_size = [32,64,128]\n",
    "optimizer = ['adam',sgd]\n",
    "for the_batch_size in Batch_size:\n",
    "    for the_dropout in dropout:\n",
    "        for the_optimizer in optimizer:\n",
    "            for the_dense_layers in dense_layers:\n",
    "                for the_lstm_layers in lstm_layers:\n",
    "                    for the_units in units:\n",
    "                        sgd = SGD(learning_rate=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "                        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=random.randint(10,100))\n",
    "                        X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "                        X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "                        model = Sequential()\n",
    "                        # model.build(input_shape=(277,277,2))\n",
    "                        #print(model.summary())\n",
    "                        #model.add(SpatialDropout1D(0.2))\n",
    "                        model.add(LSTM(the_units ,input_shape=(X_train.shape[1],X_train.shape[2]),return_sequences = True))\n",
    "                        model.add(Dropout(the_dropout))\n",
    "                        model.add(BatchNormalization())\n",
    "                        # #第二层\n",
    "                        for i in range(the_lstm_layers):\n",
    "                            model.add(LSTM(the_units,return_sequences=True))\n",
    "                            model.add(Dropout(the_dropout))\n",
    "                            model.add(BatchNormalization())\n",
    "\n",
    "                        model.add(LSTM(the_units))\n",
    "                        model.add(Dropout(the_dropout))\n",
    "                        model.add(BatchNormalization())\n",
    "                        #全连接层\n",
    "                        for i in range(the_dense_layers):\n",
    "                            model.add(Dense(the_dense_layers,activation='relu'))\n",
    "                            model.add(Dropout(the_dropout))\n",
    "                            \n",
    "                        # model.add(Flatten()) \n",
    "                        \n",
    "                        \n",
    "                        model.add(Dense(2, activation='softmax'))\n",
    "                        \n",
    "                        #sgd = SGD(learning_rate=0.01, momentum=0.9 , decay=0.1, nesterov=False)\n",
    "                        \n",
    "                        # learning_rate = 0.1\n",
    "                        # decay = 0.001\n",
    "                        # epochs = 50\n",
    "                        # batch_size = 64\n",
    "                        \n",
    "                        \n",
    "                        model.compile(  loss='binary_crossentropy',#categorical_crossentropy', binary_crossentropy\n",
    "                                        optimizer=the_optimizer, metrics=['categorical_accuracy'])\n",
    "                        print(model.summary())\n",
    "\n",
    "                        epochs = 100\n",
    "                        batch_size = the_batch_size\n",
    "                        if(the_optimizer == sgd):\n",
    "                            the_optimizer = 'sgd'\n",
    "                        filepath = './models/{categorical_accuracy:.4f}_{epoch:02d}_'+f'dropout_{the_dropout}_batch_size_{the_batch_size}_optimizer_{the_optimizer}_dense_layers_{the_dense_layers}_lstm_layers_{the_lstm_layers}_unit_{the_units}.h5'\n",
    "                        checkpoint = ModelCheckpoint(\n",
    "                                            filepath=filepath,\n",
    "                                            save_weights_only=False,\n",
    "                                            monitor='categorical_accuracy',\n",
    "                                            mode='max',\n",
    "                                            save_best_only=True)\n",
    "                        history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n",
    "                                            #callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "                                            callbacks=checkpoint)\n",
    "                        Note = open('restult.txt',mode='a+')\n",
    "                        acc = round(max(history.history['categorical_accuracy']),4)\n",
    "                        Note.write(str(acc) + ',' + f'batch_size_{the_batch_size}_optimizer_{the_optimizer}_dense_layers_{the_dense_layers}_lstm_layers_{the_lstm_layers}_unit_{the_units}\\n')\n",
    "                        print(max(history.history['categorical_accuracy']))\n",
    "                        Note.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "best_model = load_model('./models/0.8597_99_dropout_0.05_batch_size_32_optimizer_adam_dense_layers_1_lstm_layers_1_unit_16.h5')\n",
    "\n",
    "dataset = pd.read_csv('test.csv') \n",
    "cols = ['Class','Area', 'Perimeter', 'Major_Axis', 'Minor_Axis', 'Eccentricity',\n",
    "        'Eqdiasq', 'Solidity', 'Convex_Area', 'Extent', 'Aspect_Ratio',\n",
    "        'Roundness', 'Compactness', 'Shapefactor_1', 'Shapefactor_2',\n",
    "        'Shapefactor_3', 'Shapefactor_4', 'Mean_RR', 'Mean_RG', 'Mean_RB',\n",
    "        'StdDev_RR', 'StdDev_RG', 'StdDev_RB', 'Skew_RR', 'Skew_RG', 'Skew_RB',\n",
    "        'Kurtosis_RR', 'Kurtosis_RG', 'Kurtosis_RB']\n",
    "\n",
    "lis = []\n",
    "for i in range(28):\n",
    "    lis.append(27-i)\n",
    "\n",
    "Lisclass = dataset['Class'].values\n",
    "LisClassNp = []\n",
    "for i in range(len(Lisclass)):\n",
    "    if (Lisclass[i] == 'Kirmizi_Pistachio'):\n",
    "        LisClassNp.append([0,1])\n",
    "    else:\n",
    "        LisClassNp.append([1,0])\n",
    "Y = np.array(LisClassNp)\n",
    "dataset = dataset.drop(columns='Class')\n",
    "# lisindex = []\n",
    "# for i in range(len(dataset.columns)):\n",
    "#     lisindex.append(i)\n",
    "# dataset = dataset.reindex(lisindex)\n",
    "colIndex = ['Area','Minor_Axis', 'Eccentricity',\n",
    "            'Eqdiasq','Convex_Area', 'Aspect_Ratio',\n",
    "            'Compactness', 'Shapefactor_1', \n",
    "            'Shapefactor_3']\n",
    "#colIndex = FReIndex(colIndex,lis)\n",
    "dataset = pd.DataFrame(dataset,columns=colIndex)\n",
    "\n",
    "X = dataset.values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "#进行预测 make a prediction\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.9, random_state=random.randint(10,100))\n",
    "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
    "# print ('train_x.shape, train_y.shape, test_x.shape, test_y.shape')\n",
    "# print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "yhat = best_model.predict(X_test)\n",
    "# print(yhat.shape)\n",
    "print(yhat)\n",
    "print(Y_test)\n",
    "sorce = 0\n",
    "for i in range(len(Y_test)):\n",
    "    sorce += roc_auc_score(Y_test[i], yhat[i])\n",
    "\n",
    "print('ACC:',sorce/len(Y_test))\n",
    "#拆分训练集和测试集\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.10, random_state=random.randint(10,100))\n",
    "# X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "# X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33d73dfb9d58d1e4c0ed15f266f7c1fd1b5e79268076ebae9c660cb33abbd60e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
